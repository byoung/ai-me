{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a10f10b",
   "metadata": {},
   "source": [
    "# Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833c0398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup configuration\n",
    "import sys\n",
    "sys.path.append('/Users/benyoung/projects/ai-me')\n",
    "\n",
    "from src.config import config\n",
    "from IPython.display import Markdown\n",
    "from agents import trace, Runner, Agent, Tool\n",
    "\n",
    "print(f\"Using model: {config.model}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53b0ec4",
   "metadata": {},
   "source": [
    "# Download, Load, Chunk, Vectorize and Store md files in Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315075c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import src.data as _data_module\n",
    "reload(_data_module)\n",
    "from src.data import DataManager\n",
    "\n",
    "\n",
    "# Use consolidated data manager\n",
    "# For some reason, the glob pattern does not follow symlinks properly, so specify directly here\n",
    "data_manager = DataManager(\n",
    "    doc_load_local=[\"me/**/*.md\"],\n",
    "    github_repos=config.github_repos\n",
    ")\n",
    "\n",
    "# Load all repos configured in config.github_repos\n",
    "chunks = data_manager.load_and_process_all(\n",
    "    include_local=False,\n",
    "    include_github=True,\n",
    "    github_repos=config.github_repos  # Process all repos\n",
    ")\n",
    "\n",
    "print(f\"Total chunks created: {len(chunks)}\")\n",
    "\n",
    "# Create the vectorstore using DataManager\n",
    "vectorstore = data_manager.create_vectorstore(chunks, reset=True)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "data_manager.show_docs_for_file(\"faq.md\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872162c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup MCP servers\n",
    "from agents.mcp import MCPServerStdio\n",
    "import shutil\n",
    "import asyncio\n",
    "\n",
    "# Verify Docker is available\n",
    "if not shutil.which(\"docker\"):\n",
    "    raise RuntimeError(\"\"\"Docker is not available in this environment. Run this on a machine with \n",
    "                        Docker or use an IDE MCP integration.\"\"\")\n",
    "\n",
    "# Use MCP params from config\n",
    "# Override to enable MCP servers in notebook (disabled by default in config)\n",
    "all_params = [config.mcp_github_params, config.mcp_time_params]\n",
    "\n",
    "# Create MCP servers for each parameter set\n",
    "mcp_servers = []\n",
    "for i, params in enumerate(all_params):\n",
    "    try:\n",
    "        print(f\"Initializing MCP server {i+1}...\")\n",
    "        async with MCPServerStdio(params=params, client_session_timeout_seconds=30) as server:\n",
    "            tools = await server.list_tools()\n",
    "            print(f\"Server {i+1} initialized successfully with {len(tools)} tools\")\n",
    "        # Create a new instance for actual use\n",
    "        mcp_servers.append(MCPServerStdio(params))\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing MCP server {i+1}: {e}\")\n",
    "        # Continue with other servers even if one fails\n",
    "        continue\n",
    "\n",
    "print(f\"Created {len(mcp_servers)} MCP servers. Starting now...\")\n",
    "\n",
    "# Start MCP Servers\n",
    "for i, server in enumerate(mcp_servers):\n",
    "    try:\n",
    "        await server.connect()\n",
    "        print(f\"Connected to MCP server {i+1}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to MCP server {i+1}: {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1894138b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import function_tool\n",
    "\n",
    "@function_tool\n",
    "async def get_local_info(query: str) -> str:\n",
    "    \"\"\"get more context based on the subject of the question.\n",
    "    Our vector store will contain information about our personal and professional experience\n",
    "    in all things technology.\"\"\"\n",
    "    print(\"QUERY:\", query)\n",
    "    docs_content = \"\"\n",
    "    retrieved_docs = vectorstore.similarity_search_with_score(query, k=3)\n",
    "    print(f\"Retrieved {len(retrieved_docs)} documents from vector store.\")\n",
    "    for doc, score in retrieved_docs:\n",
    "        source_link = f\"https://github.com/{doc.metadata['github_repo']}/tree/main/{doc.metadata['file_path']}\\n\"\n",
    "\n",
    "        print(f\" --------------- {doc.metadata['file_path']} ({score}) ---------------\")\n",
    "        print(f\"{doc.page_content[:2000]}\")\n",
    "\n",
    "        if(score < 1.5):\n",
    "            docs_content += f\"Source: {source_link}\" + doc.page_content + \"\\n\\n\"\n",
    "\n",
    "    return docs_content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c35378f",
   "metadata": {},
   "source": [
    "# Setup Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff276c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI\n",
    "\n",
    "# Agent client already set up by config\n",
    "print(f\"Using model: {config.model}\")\n",
    "\n",
    "async def get_researcher(mcp_servers) -> Agent:\n",
    "    researcher = Agent(\n",
    "        name=\"Source Code Researcher\",\n",
    "        instructions= f\"\"\"\n",
    "            You're a source code researcher that uses your tools to gather information from github.\n",
    "            When searching source code, filter to only commits by the given GitHub username.\n",
    "            \"\"\",\n",
    "        model=config.model,\n",
    "        mcp_servers=mcp_servers,\n",
    "    )\n",
    "    return researcher\n",
    "\n",
    "\n",
    "async def get_researcher_tool(mcp_servers) -> Tool:\n",
    "    researcher = await get_researcher(mcp_servers)\n",
    "    return researcher.as_tool(\n",
    "            tool_name=\"SourceCodeResearcher\",\n",
    "            tool_description=\"\"\"\n",
    "                This tool is for searching through source code repositories. \n",
    "                Use this tool if you have a github username and repo to filter on\"\"\"\n",
    "        )\n",
    "\n",
    "researcher_tool = await get_researcher_tool(mcp_servers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578d3514",
   "metadata": {},
   "outputs": [],
   "source": [
    "ins = f\"\"\"\n",
    "You're acting as somebody who personifying Ben Young and must follow these rules:\n",
    " * If the user asks a question, use the get_local_info tool to gather more info but don't include my name in the query\n",
    " * Say you don't know if the get local info tool returns weak or no relevant info\n",
    " * don't offer follow up questions, just answer the question\n",
    " * Add inline references using shorthand links like '[1](link)' if they contain https://github.com\n",
    "\"\"\"\n",
    "print(ins)\n",
    "ai_me = Agent(\n",
    "    model=config.model,\n",
    "    name=\"ai-me\",\n",
    "    instructions=ins,\n",
    "    tools=[get_local_info],\n",
    "    # Turn off our github researcher tool until we can optimize the response time\n",
    "    # The researcher tool gives a much better response when used, but it's very slow (and expensive)\n",
    "    #tools=[researcher_tool, get_local_info],\n",
    ")\n",
    "\n",
    "with trace(\"test-1\"):\n",
    "    result = await Runner.run(ai_me, \"What do you know about LUKS?\")\n",
    "\n",
    "display(Markdown(result.final_output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8062817",
   "metadata": {},
   "outputs": [],
   "source": [
    "with trace(\"test-2\"):\n",
    "    result = await Runner.run(ai_me, \"What are you most proud of?\")\n",
    "display(Markdown(result.final_output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c409eb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "with trace(\"test-3\"):\n",
    "    result = await Runner.run(ai_me, \"Can you summarize all your blog posts into a 5-7 sentence paragraph?\")\n",
    "display(Markdown(result.final_output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c57f275",
   "metadata": {},
   "outputs": [],
   "source": [
    "with trace(\"test-4\"):\n",
    "    result = await Runner.run(ai_me, \"who is slartibartfast?\")\n",
    "display(Markdown(result.final_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169e5d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio\n",
    "\n",
    "async def chat(user_input: str, history):\n",
    "    print(\"================== USER ===================\")\n",
    "    print(user_input)\n",
    "\n",
    "    result = await Runner.run(ai_me, user_input)\n",
    "\n",
    "    print(\"================== AGENT ==================\")\n",
    "    print(result.final_output)\n",
    "    return result.final_output\n",
    "\n",
    "with gradio.Blocks(theme=gradio.themes.Ocean()) as ui:\n",
    "\n",
    "    gradio.Markdown(\"\"\"# Welcome to Ben-Bot\n",
    "                    The digital version of Ben Young, software engineer, architect, leader and technology enthusiast.\n",
    "                    The digital assistant that you never knew you needed ;)\n",
    "                    Feel free to ask me anything about my experience, skills, projects, and interests.\n",
    "                    \"\"\")\n",
    "    gradio.ChatInterface(chat, type=\"messages\")\n",
    "\n",
    "ui.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b47aa6",
   "metadata": {},
   "source": [
    "# The End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-me (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
