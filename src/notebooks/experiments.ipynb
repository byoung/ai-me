{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a10f10b",
   "metadata": {},
   "source": [
    "# Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833c0398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup configuration\n",
    "import sys\n",
    "sys.path.append('/Users/benyoung/projects/ai-me')\n",
    "\n",
    "from src.config import config\n",
    "from IPython.display import Markdown\n",
    "from agents import trace, Runner, Agent, Tool\n",
    "\n",
    "print(f\"Using model: {config.model}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53b0ec4",
   "metadata": {},
   "source": [
    "# Download, Load, Chunk, Vectorize and Store md files in Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315075c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import DataManager\n",
    "\n",
    "# Use consolidated data manager\n",
    "# For some reason, the glob pattern does not follow symlinks properly, so specify directly here\n",
    "data_manager = DataManager(\n",
    "    doc_load_local=[\"me/**/*.md\"],\n",
    "    github_repos=config.github_repos\n",
    ")\n",
    "\n",
    "# Load all repos configured in config.github_repos\n",
    "chunks = data_manager.load_and_process_all(\n",
    "    include_local=False,\n",
    "    include_github=True,\n",
    "    github_repos=config.github_repos  # Process all repos\n",
    ")\n",
    "\n",
    "print(f\"Total chunks created: {len(chunks)}\")\n",
    "\n",
    "# Create the vectorstore using DataManager\n",
    "vectorstore = data_manager.create_vectorstore(chunks, reset=True)\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef73a486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71acf495",
   "metadata": {},
   "source": [
    "# Setup MCP Servers and Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872162c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup MCP servers\n",
    "from agents.mcp import MCPServerStdio\n",
    "import shutil\n",
    "import asyncio\n",
    "\n",
    "# Verify Docker is available\n",
    "if not shutil.which(\"docker\"):\n",
    "    raise RuntimeError(\"\"\"Docker is not available in this environment. Run this on a machine with \n",
    "                        Docker or use an IDE MCP integration.\"\"\")\n",
    "\n",
    "# Use MCP params from config\n",
    "# Override to enable MCP servers in notebook (disabled by default in config)\n",
    "all_params = [config.mcp_github_params, config.mcp_time_params]\n",
    "\n",
    "# Create MCP servers for each parameter set\n",
    "mcp_servers = []\n",
    "for i, params in enumerate(all_params):\n",
    "    try:\n",
    "        print(f\"Initializing MCP server {i+1}...\")\n",
    "        async with MCPServerStdio(params=params, client_session_timeout_seconds=30) as server:\n",
    "            tools = await server.list_tools()\n",
    "            print(f\"Server {i+1} initialized successfully with {len(tools)} tools\")\n",
    "        # Create a new instance for actual use\n",
    "        mcp_servers.append(MCPServerStdio(params))\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing MCP server {i+1}: {e}\")\n",
    "        # Continue with other servers even if one fails\n",
    "        continue\n",
    "\n",
    "print(f\"Created {len(mcp_servers)} MCP servers. Starting now...\")\n",
    "\n",
    "# Start MCP Servers\n",
    "for i, server in enumerate(mcp_servers):\n",
    "    try:\n",
    "        await server.connect()\n",
    "        print(f\"Connected to MCP server {i+1}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to MCP server {i+1}: {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1894138b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import function_tool\n",
    "\n",
    "@function_tool\n",
    "async def get_local_info(query: str) -> str:\n",
    "    \"\"\"get more context based on the subject of the question.\n",
    "    Our vector store will contain information about our personal and professional experience\n",
    "    in all things technology.\"\"\"\n",
    "    print(\"QUERY:\", query)\n",
    "    retrieved_docs = vectorstore.similarity_search(query)\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "    return docs_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6faec6",
   "metadata": {},
   "source": [
    "Non-LangChain example of getting md files from a repo\n",
    "I don't like this solution as it scrapes everything, \n",
    "but it may come in handy later.\n",
    "\n",
    "```python\n",
    "from github import Github, Auth\n",
    "#\n",
    "# Get MD files from remote repos\n",
    "#\n",
    "REPOS = [\"Neosofia/corporate\"]\n",
    "g = Github(auth=Auth.Token(os.environ[\"GITHUB_PERSONAL_ACCESS_TOKEN\"]))\n",
    "documents = []\n",
    "\n",
    "for repo_name in REPOS:\n",
    "    repo = g.get_repo(repo_name)\n",
    "    contents = repo.get_contents(\"\")\n",
    "    while contents:\n",
    "        file_content = contents.pop(0)\n",
    "        if file_content.type == \"dir\":\n",
    "            contents.extend(repo.get_contents(file_content.path))\n",
    "        elif os.path.splitext(file_content.path)[1] == \".md\":\n",
    "            print(\"ADDING FILE:\", file_content)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c35378f",
   "metadata": {},
   "source": [
    "# Setup Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff276c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI\n",
    "\n",
    "# Agent client already set up by config\n",
    "print(f\"Using model: {config.model}\")\n",
    "\n",
    "async def get_researcher(mcp_servers) -> Agent:\n",
    "    researcher = Agent(\n",
    "        name=\"Source Code Researcher\",\n",
    "        instructions= f\"\"\"\n",
    "            You're a source code researcher that uses your tools to gather information from github.\n",
    "            When searching source code, filter to only commits by the given GitHub username.\n",
    "            \"\"\",\n",
    "        model=config.model,\n",
    "        mcp_servers=mcp_servers,\n",
    "    )\n",
    "    return researcher\n",
    "\n",
    "\n",
    "async def get_researcher_tool(mcp_servers) -> Tool:\n",
    "    researcher = await get_researcher(mcp_servers)\n",
    "    return researcher.as_tool(\n",
    "            tool_name=\"SourceCodeResearcher\",\n",
    "            tool_description=\"\"\"\n",
    "                This tool is for searching through source code repositories. \n",
    "                Use this tool if you have a github username and repo to filter on\"\"\"\n",
    "        )\n",
    "\n",
    "researcher_tool = await get_researcher_tool(mcp_servers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578d3514",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config.agent_prompt)\n",
    "ai_me = Agent(\n",
    "    model=config.model,\n",
    "    name=\"ai-me\",\n",
    "    instructions=config.agent_prompt,\n",
    "    tools=[get_local_info],\n",
    "    # Turn off our github researcher tool until we can optimize the response time\n",
    "    # The researcher tool gives a much better response when used, but it's very slow (and expensive)\n",
    "    #tools=[researcher_tool, get_local_info],\n",
    ")\n",
    "\n",
    "with trace(\"test-1\"):\n",
    "    result = await Runner.run(ai_me, \"Do you have experience with ReaR?\")\n",
    "\n",
    "display(Markdown(result.final_output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8062817",
   "metadata": {},
   "outputs": [],
   "source": [
    "with trace(\"test-2\"):\n",
    "    result = await Runner.run(ai_me, \"What kind of experience do you have with SQL?\")\n",
    "display(Markdown(result.final_output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c409eb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "with trace(\"test-3\"):\n",
    "    result = await Runner.run(ai_me, \"Can you summarize all your blog posts into a 5-7 sentence paragraph?\")\n",
    "display(Markdown(result.final_output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169e5d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio\n",
    "\n",
    "async def chat(user_input: str, history):\n",
    "    print(\"================== USER ===================\")\n",
    "    print(user_input)\n",
    "\n",
    "    result = await Runner.run(ai_me, user_input)\n",
    "\n",
    "    print(\"================== AGENT ==================\")\n",
    "    print(result.final_output)\n",
    "    return result.final_output\n",
    "\n",
    "view = gradio.ChatInterface(chat, type=\"messages\").launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110cf7ba",
   "metadata": {},
   "source": [
    "# LangGraph Testing (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8cf3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"github\": {\n",
    "            \"command\": \"docker\",\n",
    "            \"args\": [\n",
    "                \"run\",\"-i\",\"--rm\",\n",
    "                \"-e\",\"GITHUB_PERSONAL_ACCESS_TOKEN\",\n",
    "                \"ghcr.io/github/github-mcp-server\"\n",
    "            ],\n",
    "            \"env\": {\n",
    "                \"GITHUB_PERSONAL_ACCESS_TOKEN\": os.environ[\"GITHUB_PERSONAL_ACCESS_TOKEN\"],\n",
    "            },\n",
    "            \"transport\": \"stdio\",\n",
    "        },\n",
    "        \"time\": {\n",
    "            \"command\": \"uvx\",\n",
    "            \"args\": [\n",
    "              \"mcp-server-time\",\n",
    "              \"--local-timezone=America/New_York\"\n",
    "            ],\n",
    "            \"transport\": \"stdio\",\n",
    "        }\n",
    "    }\n",
    ")\n",
    "tools = await client.get_tools()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd4b2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from typing_extensions import List, TypedDict\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing import Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=MODEL).bind_tools(tools, parallel_tool_calls=False)\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"{system_prompt}\"),\n",
    "        MessagesPlaceholder(\"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "llm_model = prompt_template | llm\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "\n",
    "def chatbot(state: State):\n",
    "    print(state)\n",
    "    retrieved_docs = vectorstore.similarity_search(state[\"question\"])\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "    #prompt_msg = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content, \"messages\": state[\"messages\"]})\n",
    "    with trace(\"ai-ben\"):\n",
    "        result = await Runner.run(researcher, system_prompt, max_turns=30)\n",
    "        state[\"messages\"] = llm_model.invoke({\"system_prompt\": system_prompt, \"messages\": state[\"messages\"]})\n",
    "    \n",
    "    return state\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "\n",
    "#graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "#graph_builder.add_edge(START, \"retrieve\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b9a8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "\n",
    "def chat(user_input: str, history):\n",
    "    result = graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": user_input}], \"question\": user_input}, config=config)\n",
    "    return result[\"messages\"][-1].content\n",
    "\n",
    "view = gradio.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b47aa6",
   "metadata": {},
   "source": [
    "# The End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-me (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
